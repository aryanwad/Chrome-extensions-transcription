// Offscreen document for handling tab capture in Manifest V3
// Implements proper audio processing for AssemblyAI compatibility
class OffscreenAudioProcessor {
  constructor() {
    this.mediaStream = null;
    this.audioContext = null;
    this.processor = null;
    this.isProcessing = false;
    this.audioBuffer = [];
    this.chunkSize = 800; // 50ms at 16kHz (16000 * 0.05 = 800 samples)
    
    console.log('OFFSCREEN: Audio processor initialized for AssemblyAI compatibility');
    this.setupMessageListener();
  }
  
  setupMessageListener() {
    chrome.runtime.onMessage.addListener((request, sender, sendResponse) => {
      console.log('OFFSCREEN: Received message:', request);
      
      // Only handle messages that are intended for the offscreen document
      if (!this.isOffscreenMessage(request.type)) {
        return false; // Let other handlers process this message
      }
      
      switch (request.type) {
        case 'START_OFFSCREEN_CAPTURE':
          this.startCapture(request.streamId)
            .then(() => sendResponse({ success: true }))
            .catch(error => sendResponse({ success: false, error: error.message }));
          return true; // Keep message channel open for async response
          
        case 'STOP_OFFSCREEN_CAPTURE':
          this.stopCapture();
          sendResponse({ success: true });
          break;
          
        case 'PROCESS_VIDEO_FOR_AUDIO':
          this.processVideoForAudio(request.videoBuffer, request.durationMinutes)
            .then(result => sendResponse({ success: true, data: result }))
            .catch(error => sendResponse({ success: false, error: error.message }));
          return true; // Keep message channel open for async response
          
        case 'CLEANUP_OFFSCREEN':
          this.cleanup();
          sendResponse({ success: true });
          break;
          
        default:
          console.warn('OFFSCREEN: Unknown offscreen message type:', request.type);
          sendResponse({ success: false, error: `Unknown message type: ${request.type}` });
          break;
      }
    });
  }
  
  isOffscreenMessage(messageType) {
    const offscreenMessageTypes = [
      'START_OFFSCREEN_CAPTURE',
      'STOP_OFFSCREEN_CAPTURE', 
      'PROCESS_VIDEO_FOR_AUDIO',
      'CLEANUP_OFFSCREEN'
    ];
    return offscreenMessageTypes.includes(messageType);
  }
  
  async startCapture(streamId) {
    if (this.isProcessing) {
      throw new Error('Audio processing already running');
    }
    
    try {
      console.log('OFFSCREEN: Starting capture with stream ID:', streamId);
      
      // Use getUserMedia with the stream ID from tabCapture.getMediaStreamId
      // This is the proper way to use tab capture in offscreen documents
      console.log('OFFSCREEN: Requesting getUserMedia with constraints...');
      const constraints = {
        audio: {
          mandatory: {
            chromeMediaSource: "tab",
            chromeMediaSourceId: streamId,
            echoCancellation: false,
            noiseSuppression: false,
            autoGainControl: false,
            googEchoCancellation: false,
            googAutoGainControl: false,
            googNoiseSuppression: false,
            googHighpassFilter: false
          }
        }
      };
      
      console.log('OFFSCREEN: getUserMedia constraints:', JSON.stringify(constraints, null, 2));
      this.mediaStream = await navigator.mediaDevices.getUserMedia(constraints);
      
      console.log('OFFSCREEN: Got media stream with', this.mediaStream.getAudioTracks().length, 'audio tracks');
      
      // Log audio track details
      const audioTracks = this.mediaStream.getAudioTracks();
      audioTracks.forEach((track, index) => {
        console.log(`OFFSCREEN: Audio Track ${index}:`, {
          label: track.label,
          enabled: track.enabled,
          muted: track.muted,
          readyState: track.readyState,
          settings: track.getSettings()
        });
      });
      
      if (audioTracks.length === 0) {
        throw new Error('No audio tracks in the captured stream');
      }
      
      // Set up audio processing
      await this.setupAudioProcessing();
      this.isProcessing = true;
      
      console.log('‚úÖ OFFSCREEN: Audio capture started successfully');
      
    } catch (error) {
      console.error('OFFSCREEN: Failed to start capture:', error);
      this.stopCapture();
      throw error;
    }
  }
  
  async setupAudioProcessing() {
    console.log('OFFSCREEN: Setting up AudioWorklet processing for AssemblyAI...');
    
    try {
      // Create audio context with native sample rate for better quality
      // We'll downsample only for AssemblyAI processing, not for playback
      this.audioContext = new AudioContext({ 
        latencyHint: 'interactive'
      });
      
      console.log('OFFSCREEN: AudioContext created with sample rate:', this.audioContext.sampleRate);
      
      // Load the AudioWorklet module
      const audioProcessorUrl = chrome.runtime.getURL('audio-processor.js');
      console.log('OFFSCREEN: Loading AudioWorklet from URL:', audioProcessorUrl);
      
      try {
        await this.audioContext.audioWorklet.addModule(audioProcessorUrl);
        console.log('OFFSCREEN: AudioWorklet module loaded successfully');
      } catch (workletError) {
        console.error('OFFSCREEN: Failed to load AudioWorklet module:', workletError);
        throw new Error('AudioWorklet module loading failed: ' + workletError.message);
      }
      
      // Create AudioWorklet node
      try {
        this.audioWorkletNode = new AudioWorkletNode(this.audioContext, 'audio-processor');
        console.log('OFFSCREEN: AudioWorkletNode created successfully');
      } catch (nodeError) {
        console.error('OFFSCREEN: Failed to create AudioWorkletNode:', nodeError);
        throw new Error('AudioWorkletNode creation failed: ' + nodeError.message);
      }
      
      // Listen for audio data from the worklet
      this.audioWorkletNode.port.onmessage = (event) => {
        if (event.data.type === 'AUDIO_DATA') {
          this.handleAudioData(event.data);
        }
      };
      
      // Create media stream source
      const source = this.audioContext.createMediaStreamSource(this.mediaStream);
      
      // Connect: source -> AudioWorklet
      source.connect(this.audioWorkletNode);
      
      // Enable audio loopback so user can still hear the audio
      // This is safe in offscreen context as there's no speaker feedback risk
      source.connect(this.audioContext.destination);
      console.log('OFFSCREEN: Audio loopback enabled - you should hear the captured audio');
      
      // Start the worklet processing
      this.audioWorkletNode.port.postMessage({ type: 'START' });
      
      // Ensure audio context is running
      if (this.audioContext.state === 'suspended') {
        await this.audioContext.resume();
      }
      
      console.log('‚úÖ OFFSCREEN: AudioWorklet processing setup complete');
      console.log('OFFSCREEN: AudioContext state:', this.audioContext.state);
      
    } catch (error) {
      console.error('‚ùå OFFSCREEN: AudioWorklet setup error:', error);
      throw error;
    }
  }
  
  handleAudioData(audioData) {
    // Don't process audio data if we're no longer processing
    if (!this.isProcessing) {
      return;
    }
    
    // Debug: Log audio data flow
    console.log('OFFSCREEN: Handling audio data, amplitude:', audioData.amplitude);
    
    // Send audio data to background service worker
    chrome.runtime.sendMessage({
      type: 'AUDIO_DATA_FROM_OFFSCREEN',
      data: Array.from(audioData.data), // Convert Int16Array to regular array
      amplitude: audioData.amplitude,
      sampleRate: 16000
    }).then(response => {
      // If background service responds that transcription is stopped, stop processing
      if (response && !response.success && response.reason === 'transcription_stopped') {
        console.log('üõë OFFSCREEN: Background service stopped transcription, stopping audio processing');
        this.stopCapture();
      }
    }).catch(error => {
      console.error('‚ùå OFFSCREEN: Failed to send audio data to background:', error);
      // If we can't communicate with background, stop processing
      this.stopCapture();
    });
    
    // Track chunk count for internal processing
    this.chunkCount = (this.chunkCount || 0) + 1;
  }
  
  stopCapture() {
    console.log('OFFSCREEN: Stopping audio capture...');
    this.isProcessing = false;
    this.chunkCount = 0;
    
    // Stop AudioWorklet
    if (this.audioWorkletNode) {
      this.audioWorkletNode.port.postMessage({ type: 'STOP' });
      this.audioWorkletNode.disconnect();
      this.audioWorkletNode = null;
      console.log('OFFSCREEN: AudioWorklet stopped and disconnected');
    }
    
    // Close audio context
    if (this.audioContext) {
      this.audioContext.close();
      this.audioContext = null;
      console.log('OFFSCREEN: AudioContext closed');
    }
    
    // Stop media stream
    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach(track => {
        track.stop();
        console.log('OFFSCREEN: Track stopped:', track.label);
      });
      this.mediaStream = null;
    }
    
    console.log('üõë OFFSCREEN: Audio capture stopped completely');
  }
  
  async processVideoForAudio(videoBuffer, durationMinutes) {
    try {
      console.log('üé¨ OFFSCREEN: Processing video for audio extraction');
      console.log('üìä Video size:', (videoBuffer.byteLength / 1024 / 1024).toFixed(2), 'MB');
      
      // Create a blob from the video buffer
      const videoBlob = new Blob([videoBuffer], { type: 'video/mp2t' }); // TS format from m3u8
      const videoUrl = URL.createObjectURL(videoBlob);
      
      try {
        // Create video element to decode the video
        const video = document.createElement('video');
        video.src = videoUrl;
        video.muted = true;
        
        // Wait for video metadata to load
        await new Promise((resolve, reject) => {
          video.addEventListener('loadedmetadata', resolve);
          video.addEventListener('error', reject);
          video.load();
        });
        
        console.log('‚úÖ OFFSCREEN: Video metadata loaded');
        console.log('üìä Video duration:', video.duration, 'seconds');
        console.log('üìä Video dimensions:', video.videoWidth, 'x', video.videoHeight);
        
        // Create audio context for processing
        const audioContext = new (window.AudioContext || window.webkitAudioContext)({
          sampleRate: 16000 // Target sample rate for transcription
        });
        
        // Create media element source
        const source = audioContext.createMediaElementSource(video);
        
        // Create script processor for audio capture
        const bufferSize = 4096;
        const processor = audioContext.createScriptProcessor(bufferSize, 1, 1);
        
        const audioChunks = [];
        let totalSamples = 0;
        
        processor.onaudioprocess = (event) => {
          const inputBuffer = event.inputBuffer;
          const outputBuffer = event.outputBuffer;
          
          // Copy input to output (passthrough)
          for (let channel = 0; channel < outputBuffer.numberOfChannels; channel++) {
            const input = inputBuffer.getChannelData(channel);
            const output = outputBuffer.getChannelData(channel);
            
            // Convert float32 to int16 for AssemblyAI
            const int16Array = new Int16Array(input.length);
            for (let i = 0; i < input.length; i++) {
              // Clamp to [-1, 1] and convert to int16 range
              const clampedValue = Math.max(-1, Math.min(1, input[i]));
              int16Array[i] = Math.round(clampedValue * 32767);
              output[i] = input[i]; // Passthrough
            }
            
            audioChunks.push(int16Array);
            totalSamples += int16Array.length;
          }
        };
        
        // Connect audio pipeline
        source.connect(processor);
        processor.connect(audioContext.destination);
        
        // Play the video to extract audio
        video.play();
        
        // Wait for video to finish or reach duration limit
        const targetDurationSeconds = durationMinutes * 60;
        await new Promise((resolve) => {
          const checkInterval = setInterval(() => {
            if (video.ended || video.currentTime >= targetDurationSeconds) {
              clearInterval(checkInterval);
              video.pause();
              resolve();
            }
          }, 100);
        });
        
        // Cleanup
        processor.disconnect();
        source.disconnect();
        audioContext.close();
        URL.revokeObjectURL(videoUrl);
        
        // Combine all audio chunks
        const totalAudio = new Int16Array(totalSamples);
        let offset = 0;
        for (const chunk of audioChunks) {
          totalAudio.set(chunk, offset);
          offset += chunk.length;
        }
        
        console.log('‚úÖ OFFSCREEN: Audio extraction completed');
        console.log('üìä Total audio samples:', totalSamples);
        console.log('üìä Audio duration:', (totalSamples / 16000).toFixed(2), 'seconds');
        
        // Convert to ArrayBuffer for transmission
        const audioBuffer = totalAudio.buffer;
        
        return {
          audioBuffer: audioBuffer,
          sampleRate: 16000,
          samples: totalSamples,
          durationSeconds: totalSamples / 16000
        };
        
      } finally {
        URL.revokeObjectURL(videoUrl);
      }
      
    } catch (error) {
      console.error('‚ùå OFFSCREEN: Video to audio conversion failed:', error);
      throw error;
    }
  }
  
  cleanup() {
    console.log('üßπ OFFSCREEN: Performing cleanup...');
    this.stopCapture();
    // Additional cleanup if needed
  }
}

// Initialize the offscreen audio processor
const offscreenProcessor = new OffscreenAudioProcessor();

// Add global reference for debugging
window.offscreenProcessor = offscreenProcessor;

console.log('üé§ OFFSCREEN: AssemblyAI-compatible audio processor loaded at:', new Date().toISOString());